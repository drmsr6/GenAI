{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we are primarly used RAG(Retrieval Augmented Generation) to overcome traditional LLM limitations.The RAG primarly consist of three aspects:\n",
        "Indexing\n",
        "Retrieval\n",
        "Generation\n",
        "Indexing happens ahead of time, and allows you to quickly look up relevant information at query-time. When a query comes in, you retrieve relevant documents, combine them with your instructions and the user's query, and have the LLM generate a tailored answer in natural language using the supplied information. All these will help us to retrieve reason behind earth quakes.\n",
        "**Documents are the items that are in the database. They are inserted first, and later retrieved. Queries are the textual search terms and can be simple keywords or textual descriptions of the desired documents**"
      ],
      "metadata": {
        "id": "0TrQs9VeKDYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the Gemini API to create a vector database, retrieve answers to questions from the database and generate a final answer.\n",
        "**Setup**\n",
        "First, installing ChromaDB and the Gemini API Python SDK with required libraries."
      ],
      "metadata": {
        "id": "QZ0aGNvUK6Yy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukb6pxCoBjVq"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q \"google-genai>=1.0.0\"\n",
        "%pip install -q chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "import chromadb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.display import Markdown\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings"
      ],
      "metadata": {
        "id": "aCWx8IMNBuhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the key available through colab(Left hand side SIGN) -secrets, & add  key or enable it for this notebook."
      ],
      "metadata": {
        "id": "ZzcqgcSfLuAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=\"Explain Earthquakes to me like I'm a kid.\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "WX3sXGQEB_my"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploring available models**\n",
        "We are using the embedContent API method to calculate embeddings in this notebook. We will check existing embeded models for processing text."
      ],
      "metadata": {
        "id": "xdvFt-KiMEWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in client.models.list():\n",
        "  if 'embedContent' in m.supported_actions:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "id": "j8-mnHXbCJlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are creating small set of documents below and there by creates an embedding database.The below 4 documents will help us to understand the resons behinfd earthquakes and specific reason of Mynmar & Thailand earthquake."
      ],
      "metadata": {
        "id": "Gxa-B29zMiQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DOCUMENT1 = \"\"\"\n",
        "Myanmar is considered to be one of the most geologically \"active\" areas in the world because it sits on top of the convergence of four of these tectonic plates - the Eurasian plate, the Indian plate, the Sunda plate and the Burma microplate.\n",
        "\"\"\"\n",
        "DOCUMENT2 = \"\"\"\n",
        "There is a major fault called the Sagaing fault, which cuts right through Myanmar north to south and is more than 1,200km (746 miles) long.As the plates move past each other, they can become stuck, building friction until it is suddenly released and the earth shifts, causing an earthquake.\n",
        "\"\"\"\n",
        "DOCUMENT3 = \"\"\"\n",
        "Early data suggests that the movement that caused Friday's 7.7-magnitude earthquake was a \"strike-slip\" - where two blocks move horizontally along each other.Because the fracture \"unzipped\" towards the south, it also directed this piled up energy towards the Thai capital, Bangkok, and this is why the earthquake had such an impact so far away.\n",
        "\"\"\"\n",
        "DOCUMENT4 = \"\"\"\n",
        "plates move past each other horizontally at different speeds. While this causes “strike slip” quakes that are normally less powerful than those seen in “subduction zones” like Sumatra, where one plate slides under another, they can still reach magnitudes of 7 to 8\n",
        "\"\"\"\n",
        "documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3,DOCUMENT4]\n"
      ],
      "metadata": {
        "id": "UezKJAYLCPHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are Creating a custom function to generate embeddings with the Gemini API. In this task, we are implementing a **retrieval system**, so the **task_type** for generating the document embeddings is retrieval_document. Later, you will use **retrieval_query** for the query embeddings."
      ],
      "metadata": {
        "id": "vdzsMVKdN1lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai import types\n",
        "\n",
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "  def __call__(self, input: Documents) -> Embeddings:\n",
        "    EMBEDDING_MODEL_ID = \"models/embedding-001\"  # @param [\"models/embedding-001\", \"models/text-embedding-004\", \"models/gemini-embedding-exp-03-07\", \"models/gemini-embedding-exp\"] {\"allow-input\": true, \"isTemplate\": true}\n",
        "    title = \"Custom query\"\n",
        "    response = client.models.embed_content(\n",
        "        model=EMBEDDING_MODEL_ID,\n",
        "        contents=input,\n",
        "        config=types.EmbedContentConfig(\n",
        "          task_type=\"retrieval_document\",\n",
        "          title=title\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return response.embeddings[0].values"
      ],
      "metadata": {
        "id": "dlVU9tJKCzoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are creating a Chroma database client which uses the **GeminiEmbeddingFunction** and then populating the database with the documents we defined above."
      ],
      "metadata": {
        "id": "bETVjIeKOILL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_chroma_db(documents, name):\n",
        "  chroma_client = chromadb.Client()\n",
        "  db = chroma_client.create_collection(\n",
        "      name=name,\n",
        "      embedding_function=GeminiEmbeddingFunction()\n",
        "  )\n",
        "\n",
        "  for i, d in enumerate(documents):\n",
        "    db.add(\n",
        "      documents=d,\n",
        "      ids=str(i)\n",
        "    )\n",
        "  return db"
      ],
      "metadata": {
        "id": "AxrE1nBfC9ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are creating vector db for retrieval"
      ],
      "metadata": {
        "id": "rb8N-fNVPTu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = create_chroma_db(documents, \"Recent_Earthquakes_Mynmar_Thailand\")"
      ],
      "metadata": {
        "id": "nSx1uT4EDCxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are retrieving the stored data — then formatting it into a nice pandas DataFrame for easy inspection."
      ],
      "metadata": {
        "id": "Qa8xMOduPmE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = db.get(include=['documents', 'embeddings'])\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"IDs\": sample_data['ids'][:4],\n",
        "    \"Documents\": sample_data['documents'][:4],\n",
        "    \"Embeddings\": [str(emb)[:100] + \"...\" for emb in sample_data['embeddings'][:4]]  # Truncate embeddings\n",
        "})\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "d-Cz0n7sDPIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The belwo function is designed to retrieve the most relevant passage from your stored documents based on a text query using semantic similarity."
      ],
      "metadata": {
        "id": "Q4GZTsVxQAqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_passage(query, db):\n",
        "  passage = db.query(query_texts=[query], n_results=1)['documents'][0][0]\n",
        "  return passage"
      ],
      "metadata": {
        "id": "nHbDhe73DgR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below searches the vector database (db) for the document most semantically similar to the phrase \"major fault\".\n",
        "\n",
        "The function returns the top matching text passage, which is saved into the variable passage."
      ],
      "metadata": {
        "id": "p4oL9s0tQSWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform embedding search\n",
        "passage = get_relevant_passage(\"major fault\", db)\n",
        "Markdown(passage)"
      ],
      "metadata": {
        "id": "iGGX7vjMDpZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Augmented generation**: Answer the question¶\n",
        "Now that you have found a relevant passage from the set of documents (the retrieval step), you can now assemble a generation prompt to have the Gemini API generate a final answer. Note that in this example only a single passage was retrieved.Here we are adding useful prompts for retrieval purpose  "
      ],
      "metadata": {
        "id": "dmqH5HDHQoFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(query, relevant_passage):\n",
        "  escaped = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
        "  prompt = (\"\"\"\n",
        "    You are a helpful and informative bot that answers questions using\n",
        "    text from the reference passage included below.\n",
        "    Be sure to respond in a complete sentence, being comprehensive,\n",
        "    including all relevant background information.\n",
        "    However, you are talking to a non-technical audience, so be sure to\n",
        "    break down complicated concepts and strike a friendly\n",
        "    and converstional tone. If the passage is irrelevant to the answer,\n",
        "    you may ignore it.\n",
        "    QUESTION: '{query}'\n",
        "    PASSAGE: '{relevant_passage}'\n",
        "\n",
        "    ANSWER:\n",
        "  \"\"\").format(query=query, relevant_passage=escaped)\n",
        "\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "aff5iTXvD1Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"why plates moving?\"\n",
        "prompt = make_prompt(query, passage)\n",
        "Markdown(prompt)"
      ],
      "metadata": {
        "id": "KO_GPD6lEHUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are using **generate_content** method to to get an answer to the question."
      ],
      "metadata": {
        "id": "jjoNl7dxRJg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"gemini-2.0-flash\"  # @param [\"gemini-2.0-flash-lite\", \"gemini-2.0-flash\", \"gemini-2.5-pro-exp-03-25\"] {\"allow-input\": true, \"isTemplate\": true}\n",
        "answer = client.models.generate_content(\n",
        "    model = MODEL_ID,\n",
        "    contents = prompt\n",
        ")\n",
        "Markdown(answer.text)"
      ],
      "metadata": {
        "id": "KNIJWNYBEWU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above excercise will help us to Q&A nd relevance of RAG for retreival generation."
      ],
      "metadata": {
        "id": "tXAjHFwvRUyD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pMC6wsfEmjT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
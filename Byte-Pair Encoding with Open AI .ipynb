{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOUGMKNQXKbubd0s+wPlijM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VrGrbeJEcJUT","executionInfo":{"status":"ok","timestamp":1745478330879,"user_tz":-330,"elapsed":45,"user":{"displayName":"Dr Sridhar Mutluri","userId":"10430316746332583755"}}},"outputs":[],"source":[]},{"cell_type":"markdown","source":["**Byte Pair Encoding** is a subword tokenization technique that breaks text into smaller pieces (called tokens), which helps models deal with rare words, new words, and different languages.1. It Start with characters: Every word is split into individual characters.Then it Find frequent pairs then it Merge the pair and will Repeat same if require.\n","Below notebook walk through a Byte Pair Encoding (BPE) example using OpenAIâ€™s tokenizer in Google Colab.OpenAI provides a tokenizer through the tiktoken library"],"metadata":{"id":"31766ZWCcKa3"}},{"cell_type":"code","source":["#Install Required Package\n","!pip install tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BN0ZcXUwfVNS","executionInfo":{"status":"ok","timestamp":1745478337952,"user_tz":-330,"elapsed":7071,"user":{"displayName":"Dr Sridhar Mutluri","userId":"10430316746332583755"}},"outputId":"f6cdbcca-e3fc-4ca4-8c8a-adeed576d224"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"]}]},{"cell_type":"code","source":["#Tokenize Text Using OpenAIâ€™s Tokenizer (BPE)\n","import tiktoken\n","\n","# Choose model tokenizer: 'cl100k_base' for GPT-4/3.5-turbo, or 'gpt2' for earlier models\n","encoding = tiktoken.get_encoding(\"cl100k_base\")"],"metadata":{"id":"mjhZvmgsfdn3","executionInfo":{"status":"ok","timestamp":1745478338479,"user_tz":-330,"elapsed":525,"user":{"displayName":"Dr Sridhar Mutluri","userId":"10430316746332583755"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Using Example prompt from credit card holders\n","text = \"Segment credit card customers based on income and credit limit.\"\n","\n","# Encode (BPE)\n","tokens = encoding.encode(text)\n","decoded = [encoding.decode_single_token_bytes(tok).decode(errors='ignore') for tok in tokens]\n","\n","# Display tokens\n","print(\"ðŸ§± Token IDs:\", tokens)\n","print(\"ðŸ§© Token Pieces:\", decoded)\n","print(\"ðŸ”¢ Total Tokens:\", len(tokens))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sVANNEnCfvCB","executionInfo":{"status":"ok","timestamp":1745478338512,"user_tz":-330,"elapsed":23,"user":{"displayName":"Dr Sridhar Mutluri","userId":"10430316746332583755"}},"outputId":"d07bcf11-50ef-44e2-9f2c-9d9a56c6f7f5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ§± Token IDs: [21766, 6807, 3786, 6444, 3196, 389, 8070, 323, 6807, 4017, 13]\n","ðŸ§© Token Pieces: ['Segment', ' credit', ' card', ' customers', ' based', ' on', ' income', ' and', ' credit', ' limit', '.']\n","ðŸ”¢ Total Tokens: 11\n"]}]},{"cell_type":"markdown","source":["The above shows exactly how BPE merges character groups into subwords.If we use OpenAI's GPT-4/3.5 API, you can use token count to estimate cost:"],"metadata":{"id":"mkbGfaoNg55K"}},{"cell_type":"code","source":["# Estimate cost (GPT-4 8k context)\n","cost_per_1k = 0.03  # e.g., $0.03 per 1K prompt tokens\n","token_count = len(tokens)\n","cost = (token_count / 1000) * cost_per_1k\n","print(f\"ðŸ’° Estimated prompt cost: ${cost:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EJl2PFMog9T9","executionInfo":{"status":"ok","timestamp":1745478338530,"user_tz":-330,"elapsed":10,"user":{"displayName":"Dr Sridhar Mutluri","userId":"10430316746332583755"}},"outputId":"f37da4a9-8dd9-46c5-f656-587d9dd09b42"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ’° Estimated prompt cost: $0.0003\n"]}]}]}